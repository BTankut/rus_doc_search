import os
import streamlit as st
from typing import List, Dict
from langdetect import detect
from more_itertools import chunked
from pathlib import Path
import re
from datetime import datetime
import time
import PyPDF2
import io
import openai
from dotenv import load_dotenv
import json

# .env dosyasÄ±nÄ± yÃ¼kle
load_dotenv()

# OpenAI yapÄ±landÄ±rmasÄ±
openai.api_base = "https://openrouter.ai/api/v1"
openai.api_key = os.getenv("OPENROUTER_API_KEY")

# Dil Ã§evirileri
TRANSLATIONS = {
    "tr": {
        "title": "ðŸ“š RusÃ§a DokÃ¼man Arama Sistemi",
        "description": "PDF ve TXT formatÄ±ndaki RusÃ§a dokÃ¼manlarÄ± yÃ¼kleyin ve arama yapÄ±n.",
        "upload_title": "ðŸ“ DokÃ¼man YÃ¼kleme",
        "upload_label": "RusÃ§a dokÃ¼manlarÄ± yÃ¼kleyin (PDF/TXT)",
        "search_history": "ðŸ” Arama GeÃ§miÅŸi",
        "search_tab": "ðŸ” DokÃ¼man Arama",
        "ai_tab": "ðŸ¤– Yapay Zeka Sohbet",
        "search_input": "ðŸ” Arama yapmak iÃ§in bir kelime veya cÃ¼mle girin:",
        "ai_input": "ðŸ’­ DokÃ¼manlar hakkÄ±nda bir soru sorun:",
        "no_results": "âš ï¸ SonuÃ§ bulunamadÄ±.",
        "results_found": "âœ¨ {} sonuÃ§ bulundu!",
        "result_title": "ðŸ“„ SonuÃ§ {} - {} (ParÃ§a {})",
        "doc_stats": "ðŸ“Š DokÃ¼man boyutu: {} | {} karakter",
        "upload_first": "âš ï¸ Ã–nce dokÃ¼man yÃ¼klemelisiniz!",
        "ai_thinking": "ðŸ¤– Yapay zeka dÃ¼ÅŸÃ¼nÃ¼yor...",
        "error": "ÃœzgÃ¼nÃ¼m, bir hata oluÅŸtu: {}"
    },
    "en": {
        "title": "ðŸ“š Russian Document Search System",
        "description": "Upload Russian documents in PDF and TXT format and search through them.",
        "upload_title": "ðŸ“ Document Upload",
        "upload_label": "Upload Russian documents (PDF/TXT)",
        "search_history": "ðŸ” Search History",
        "search_tab": "ðŸ” Document Search",
        "ai_tab": "ðŸ¤– AI Chat",
        "search_input": "ðŸ” Enter a word or phrase to search:",
        "ai_input": "ðŸ’­ Ask a question about the documents:",
        "no_results": "âš ï¸ No results found.",
        "results_found": "âœ¨ {} results found!",
        "result_title": "ðŸ“„ Result {} - {} (Chunk {})",
        "doc_stats": "ðŸ“Š Document size: {} | {} characters",
        "upload_first": "âš ï¸ Please upload documents first!",
        "ai_thinking": "ðŸ¤– AI is thinking...",
        "error": "Sorry, an error occurred: {}"
    },
    "ru": {
        "title": "ðŸ“š Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð¿Ð¾Ð¸ÑÐºÐ° Ñ€ÑƒÑÑÐºÐ¸Ñ… Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²",
        "description": "Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ Ñ€ÑƒÑÑÐºÐ¸Ðµ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ PDF Ð¸ TXT Ð¸ Ð²Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ñ‚Ðµ Ð¿Ð¾Ð¸ÑÐº.",
        "upload_title": "ðŸ“ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²",
        "upload_label": "Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ Ñ€ÑƒÑÑÐºÐ¸Ðµ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ (PDF/TXT)",
        "search_history": "ðŸ” Ð˜ÑÑ‚Ð¾Ñ€Ð¸Ñ Ð¿Ð¾Ð¸ÑÐºÐ°",
        "search_tab": "ðŸ” ÐŸÐ¾Ð¸ÑÐº Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²",
        "ai_tab": "ðŸ¤– Ð§Ð°Ñ‚ Ñ Ð˜Ð˜",
        "search_input": "ðŸ” Ð’Ð²ÐµÐ´Ð¸Ñ‚Ðµ ÑÐ»Ð¾Ð²Ð¾ Ð¸Ð»Ð¸ Ñ„Ñ€Ð°Ð·Ñƒ Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ°:",
        "ai_input": "ðŸ’­ Ð—Ð°Ð´Ð°Ð¹Ñ‚Ðµ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ…:",
        "no_results": "âš ï¸ Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹.",
        "results_found": "âœ¨ ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {} Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²!",
        "result_title": "ðŸ“„ Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ {} - {} (Ð§Ð°ÑÑ‚ÑŒ {})",
        "doc_stats": "ðŸ“Š Ð Ð°Ð·Ð¼ÐµÑ€ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°: {} | {} ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²",
        "upload_first": "âš ï¸ Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹!",
        "ai_thinking": "ðŸ¤– Ð˜Ð˜ Ð´ÑƒÐ¼Ð°ÐµÑ‚...",
        "error": "Ð˜Ð·Ð²Ð¸Ð½Ð¸Ñ‚Ðµ, Ð¿Ñ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð° Ð¾ÑˆÐ¸Ð±ÐºÐ°: {}"
    }
}

class DocumentSearchSystem:
    def __init__(self):
        self.documents: List[Dict] = []
        self.doc_dir = "documents"
        self.search_history = []
        self.model = None
        self.embeddings = {}
        
        # DokÃ¼man dizinini oluÅŸtur
        os.makedirs(self.doc_dir, exist_ok=True)
        
        # Model yÃ¼kleniyor mesajÄ±
        if not st.session_state.get('model_loaded', False):
            with st.spinner('ðŸ¤– Yapay zeka modeli yÃ¼kleniyor... (Ä°lk aÃ§Ä±lÄ±ÅŸta biraz zaman alabilir)'):
                self.load_model()
                st.session_state.model_loaded = True
    
    def load_model(self):
        """RusÃ§a dil modelini yÃ¼kle"""
        self.model = SentenceTransformer('DeepPavlov/rubert-base-cased-sentence')
        
    def get_embedding(self, text: str) -> np.ndarray:
        """Metin iÃ§in vektÃ¶r oluÅŸtur"""
        return self.model.encode(text, convert_to_tensor=True)
    
    def compute_similarity(self, query_embedding: torch.Tensor, text_embedding: torch.Tensor) -> float:
        """Benzerlik skorunu hesapla"""
        return torch.nn.functional.cosine_similarity(query_embedding.unsqueeze(0), 
                                                   text_embedding.unsqueeze(0)).item()
    
    def extract_pdf_text(self, file) -> str:
        """PDF dosyasÄ±ndan metin Ã§Ä±kar"""
        try:
            pdf_reader = PyPDF2.PdfReader(file)
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
            return text
        except Exception as e:
            st.error(f"âŒ PDF okuma hatasÄ±: {str(e)}")
            return ""
        
    def save_document(self, file) -> bool:
        try:
            # Dosya uzantÄ±sÄ±nÄ± kontrol et
            file_ext = Path(file.name).suffix.lower()
            
            if file_ext == '.pdf':
                content = self.extract_pdf_text(file)
                if not content:
                    return False
            elif file_ext == '.txt':
                content = file.read().decode('utf-8')
            else:
                st.warning(f"âš ï¸ {file.name} desteklenmeyen dosya formatÄ±! Sadece .pdf ve .txt dosyalarÄ± kabul edilir.")
                return False
            
            # RusÃ§a kontrolÃ¼
            if detect(content) == 'ru':
                # DosyayÄ± kaydet
                save_path = os.path.join(self.doc_dir, file.name)
                
                # PDF ise orijinal dosyayÄ± kopyala
                if file_ext == '.pdf':
                    file.seek(0)  # Dosya iÅŸaretÃ§isini baÅŸa al
                    with open(save_path, 'wb') as f:
                        f.write(file.read())
                else:
                    with open(save_path, 'w', encoding='utf-8') as f:
                        f.write(content)
                    
                self.documents.append({
                    'name': file.name,
                    'path': save_path,
                    'content': content,
                    'size': len(content.encode('utf-8')),
                    'char_count': len(content),
                    'type': 'PDF' if file_ext == '.pdf' else 'TXT',
                    'upload_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                })
                return True
            else:
                st.warning(f"âš ï¸ {file.name} RusÃ§a deÄŸil!")
                return False
        except Exception as e:
            st.error(f"âŒ Hata: {file.name} dosyasÄ± iÅŸlenirken hata oluÅŸtu - {str(e)}")
            return False
            
    def highlight_text(self, text: str, query: str) -> str:
        """Metinde arama sorgusunu vurgula"""
        if not query:
            return text
            
        pattern = re.compile(f'({re.escape(query)})', re.IGNORECASE)
        return pattern.sub(r'**\1**', text)
            
    def search_documents(self, query: str, chunk_size: int = 500) -> List[Dict]:
        """DokÃ¼manlarda arama yap"""
        if not query.strip():
            return []
            
        # Arama geÃ§miÅŸine ekle
        self.search_history.append({
            'query': query,
            'time': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        })
        self.search_history = self.search_history[-5:]  # Son 5 aramayÄ± tut
        
        results = []
        query_embedding = self.get_embedding(query)
        
        for doc in self.documents:
            content = doc['content']
            chunks = list(chunked(content, chunk_size))
            
            for i, chunk in enumerate(chunks):
                chunk_text = ''.join(chunk)
                
                # Chunk'Ä±n vektÃ¶rÃ¼nÃ¼ hesapla veya cache'den al
                chunk_key = f"{doc['name']}_{i}"
                if chunk_key not in self.embeddings:
                    self.embeddings[chunk_key] = self.get_embedding(chunk_text)
                chunk_embedding = self.embeddings[chunk_key]
                
                # Benzerlik skorunu hesapla
                similarity = self.compute_similarity(query_embedding, chunk_embedding)
                
                # Benzerlik skoru 0.5'ten bÃ¼yÃ¼kse sonuÃ§lara ekle
                if similarity > 0.5:
                    results.append({
                        'document': doc['name'],
                        'text': chunk_text,
                        'similarity': similarity,
                        'chunk_index': i,
                        'size': doc['size'],
                        'char_count': doc['char_count'],
                        'type': doc.get('type', 'TXT')
                    })
        
        # Benzerlik skoruna gÃ¶re sÄ±rala
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results[:10]  # En iyi 10 sonucu dÃ¶ndÃ¼r

    def ask_ai(self, question: str, context: str, lang: str = "tr") -> str:
        """GPT-4'e soru sor"""
        try:
            system_prompts = {
                "tr": "Sen RusÃ§a dokÃ¼manlar konusunda uzman bir asistansÄ±n. Verilen baÄŸlamÄ± kullanarak sorularÄ± TÃ¼rkÃ§e olarak detaylÄ± bir ÅŸekilde cevaplayabilirsin.",
                "en": "You are an expert assistant specializing in Russian documents. You can answer questions in English using the given context.",
                "ru": "Ð’Ñ‹ - Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚-ÑÐºÑÐ¿ÐµÑ€Ñ‚ Ð¿Ð¾ Ñ€ÑƒÑÑÐºÐ¸Ð¼ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ð¼. Ð’Ñ‹ Ð¼Ð¾Ð¶ÐµÑ‚Ðµ Ð¾Ñ‚Ð²ÐµÑ‡Ð°Ñ‚ÑŒ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼ ÑÐ·Ñ‹ÐºÐµ, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð½Ñ‹Ð¹ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚."
            }
            
            response = openai.ChatCompletion.create(
                model="openai/gpt-4",
                messages=[
                    {"role": "system", "content": system_prompts[lang]},
                    {"role": "user", "content": f"Context:\n{context}\n\nQuestion: {question}"}
                ],
                headers={
                    "HTTP-Referer": "https://github.com/BTankut/rus_doc_search",
                    "X-Title": "Russian Document Search"
                }
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            return TRANSLATIONS[lang]["error"].format(str(e))

def format_size(size_bytes: int) -> str:
    """Boyutu okunabilir formata Ã§evir"""
    for unit in ['B', 'KB', 'MB', 'GB']:
        if size_bytes < 1024:
            return f"{size_bytes:.1f} {unit}"
        size_bytes /= 1024
    return f"{size_bytes:.1f} TB"

def main():
    # Dil seÃ§imi
    if "lang" not in st.session_state:
        st.session_state.lang = "tr"
        
    lang = st.sidebar.selectbox(
        "ðŸŒ Language / Ð¯Ð·Ñ‹Ðº / Dil",
        ["TÃ¼rkÃ§e", "English", "Ð ÑƒÑÑÐºÐ¸Ð¹"],
        index=["tr", "en", "ru"].index(st.session_state.lang)
    )
    
    # Dil kodunu gÃ¼ncelle
    st.session_state.lang = {"TÃ¼rkÃ§e": "tr", "English": "en", "Ð ÑƒÑÑÐºÐ¸Ð¹": "ru"}[lang]
    
    # Ã‡evirileri al
    t = TRANSLATIONS[st.session_state.lang]
    
    st.title(t["title"])
    st.write(t["description"])
    
    system = DocumentSearchSystem()
    
    # Sol sidebar
    with st.sidebar:
        st.header(t["upload_title"])
        uploaded_files = st.file_uploader(
            t["upload_label"],
            type=["txt", "pdf"],
            accept_multiple_files=True
        )
        
        if uploaded_files:
            for file in uploaded_files:
                system.save_document(file)
                
        st.divider()
        st.header(t["search_history"])
        for h in system.search_history:
            st.text(f"ðŸ•’ {h['time']}\nâ”” {h['query']}")
    
    # Ana iÃ§erik
    tab1, tab2 = st.tabs([t["search_tab"], t["ai_tab"]])
    
    # Arama sekmesi
    with tab1:
        query = st.text_input(t["search_input"])
        
        if query:
            results = system.search_documents(query)
            
            if not results:
                st.warning(t["no_results"])
            else:
                st.success(t["results_found"].format(len(results)))
                
                for i, result in enumerate(results, 1):
                    with st.expander(
                        t["result_title"].format(i, result['document'], result['chunk_index'] + 1)
                    ):
                        st.markdown(f"""
                        {result['text']}
                        
                        ---
                        {t["doc_stats"].format(format_size(result['size']), result['char_count'])}
                        """)
    
    # Yapay Zeka sekmesi
    with tab2:
        if not system.documents:
            st.warning(t["upload_first"])
        else:
            question = st.text_input(t["ai_input"])
            
            if question:
                # TÃ¼m dokÃ¼manlarÄ± birleÅŸtir
                all_docs = "\n---\n".join([
                    f"Document: {doc['name']}\nContent: {doc['content'][:1000]}"
                    for doc in system.documents
                ])
                
                with st.spinner(t["ai_thinking"]):
                    answer = system.ask_ai(question, all_docs, st.session_state.lang)
                    st.write(answer)

if __name__ == "__main__":
    main()
